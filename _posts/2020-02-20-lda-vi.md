---
title: "LDAで変分推論"
permalink: 2020-02-20-lda-vi.html
sidebar: blog_sidebar
---

\begin{align}
\newcommand{\b}[1]{\mathbf{#1}}
\newcommand{\bo}[1]{\boldsymbol{#1}} \nonumber
\end{align}

**※今回の記事は実装を含んでいません．**

[ベイズ推論による機械学習入門][bayes-book]より，LDAによる変分推論を見ていきます．これは5.4節の内容です．

## モデル

まずは記号と変数について列挙しておきます．

- 語彙の総数（種類数）$V$
- 文書$d$の$n$番目の単語$\b{w}\_{n, d} \in \\{0, 1 \\}^V \; \left( \sum_{v = 1}^V w\_{d, n, v} = 1 \right)$
- 文書$d$は$N$個の単語集合$\b{W}_d = \\{ \b{w}\_{d, 1}, \ldots, \b{w}\_{d, N} \\}$として表現
- $D$個の文書集合$\b{W} = \\{ \b{W}_1, \ldots, \b{W}_D \\}$
- トピックの総数$K$
- $k$番目のトピックにおける各単語の出現比率$\bo{\phi}\_k \in (0, 1)^V \; \left( \sum_{v = 1}^V \phi\_{k, v} = 1 \right)$
- 文書$d$のトピック比率$\bo{\theta}_d \in (0, 1)^K \; \left( \sum\_{k = 1}^K \theta\_{d, k} = 1 \right)$
- 文書$d$中の$n$番目の単語のトピック割り当て$\b{z}\_{d, n} \in \\{0, 1 \\}^K \; \left( \sum\_{k = 1}^K z\_{d, n, k} = 1 \right)$

さて，モデルを考えていきます．単語$\b{w}\_{d, n}$およびトピック割り当て$\b{z}\_{d, n}$はカテゴリ分布によって生成されるとします．

\begin{align}
p(\b{w}\_{d, n} | \b{z}\_{d, n}, \bo{\Phi}) &= \prod_{k = 1}^K \mathrm{Cat} (\b{w}\_{d, n} | \bo{\phi}_k)^{z\_{d, n, k}} \newline
p(\b{z}\_{d, n} | \bo{\theta}_d) &= \mathrm{Cat} (\b{z}\_{d, n} | \bo{\theta}_d) \newline
\end{align}

パラメータの事前分布はカテゴリ分布に対する事前共役分布であるディリクレ分布を選択します．

\begin{align}
p(\bo{\theta}_d) &= \mathrm{Dir} (\bo{\theta}_d | \bo{\alpha}) \newline
p(\bo{\phi}_k) &= \mathrm{Dir} (\bo{\phi}_k | \bo{\beta})
\end{align}

この生成過程をグラフィカルモデルとして表現しておきます．

![](data/2020-02-20-lda-vi/lda-graphical-model.png)

$\b{w}\_{d, n}, \b{z}\_{d, n}, \bo{\phi}_k, \bo{\theta}_d$のそれぞれに対する集合を$\b{W}, \b{Z}, \bo{\Phi}, \bo{\Theta}$と表記します．
すると，同時分布は

\begin{align}
& p (\b{W}, \b{Z}, \bo{\Phi}, \bo{\Theta}) \newline
&= p(\b{W} | \b{Z}, \bo{\Phi}) p(\b{Z} | \bo{\Theta}) p(\bo{\Phi}) p(\bo{\Theta}) \newline
&= \left\\{ \prod_{d = 1}^D \prod_{n = 1}^N p(\b{w}\_{d, n} | \b{z}\_{d, n}, \bo{\Phi}) \right\\} \left\\{ \prod_{d = 1}^D \prod_{n = 1}^N p(\b{z}\_{d, n} | \bo{\theta}_d) \right\\} \left\\{ \prod\_{k = 1}^K p(\bo{\phi}_k) \right\\} \left\\{ \prod\_{d = 1}^D p(\bo{\theta}_d) \right\\}
\end{align}

となります．


## 変分推論

$\b{W}$が与えられたときの他の変数の事後分布$p(\b{Z}, \bo{\Phi}, \bo{\Theta} \| \b{W})$を計算することが目標です．

ここでは事後分布を潜在変数$\b{Z}$とその他の変数の2つに分解できるとして，事後分布を近似します．

\begin{align}
p(\b{Z}, \bo{\Phi}, \bo{\Theta} | \b{W}) \approx q(\b{Z}) q(\bo{\Phi}, \bo{\Theta})
\end{align}

分解した各分布は次のようになります．$q(\b{Z})$は$\b{Z}$を含まない分布を定数とみなすことで

\begin{align}
\ln q(\b{Z}) 
&= \left< \ln p (\b{W}, \b{Z}, \bo{\Phi}, \bo{\Theta}) \right>\_{q(\bo{\Phi}, \bo{\Theta})} + \mathrm{const.} \newline
&= \left< \ln p(\b{W} | \b{Z}, \bo{\Phi}) p(\b{Z} | \bo{\Theta}) p(\bo{\Phi}) p(\bo{\Theta}) \right>\_{q(\bo{\Phi}, \bo{\Theta})} + \mathrm{const.} \newline
&= \left< \ln p(\b{W} | \b{Z}, \bo{\Phi}) \right>\_{q(\bo{\Phi})} + \left< \ln p(\b{Z} | \bo{\Theta}) \right>\_{q(\bo{\Theta})} + \mathrm{const.} \newline
&= \sum_{d = 1}^D \sum_{n = 1}^N \left\\{ \left< \ln p(\b{w}\_{d, n} | \b{z}\_{d, n}, \bo{\Phi}) \right>\_{q(\bo{\Phi})} + \left< \ln p(\b{z}\_{d, n} | \bo{\theta}_d ) \right>\_{q(\bo{\theta}_d)} \right\\} + \mathrm{const.}
\end{align}

となります．$q(\bo{\Theta}, \bo{\Phi})$は同様に

\begin{align}
\ln q(\bo{\Theta}, \bo{\Phi}) = \left< \ln p(\b{W} | \b{Z}, \bo{\Phi}) \right>\_{q(\b{Z})} + \left< \ln p(\b{Z} | \bo{\Theta}) \right>\_{q(\b{Z})} + \ln p(\bo{\Phi}) + \ln p(\bo{\Theta}) + \mathrm{const.}
\end{align}

です．
詳細に見ていきましょう．まず$q(\b{Z})$は$\b{z}\_{d, n}$毎の要素に分解されているので$q(\b{z}\_{d, n})$が分かれば十分です．

\begin{align}
\ln q(\b{z}\_{d, n}) = 
\left< \ln p(\b{w}\_{d, n} | \b{z}\_{d, n}, \bo{\Phi}) \right>\_{q(\bo{\Phi})} + \left< \ln p(\b{z}\_{d, n} | \bo{\theta}_d ) \right>\_{q(\bo{\theta}_d)} + \mathrm{const.}
\end{align}

ここで
\begin{align}
\left< \ln p(\b{w}\_{d, n} | \b{z}\_{d, n}, \bo{\Phi}) \right>\_{q(\bo{\Phi})} 
&= \left< \ln \prod_{k = 1}^K \mathrm{Cat} (\b{w}\_{d, n} | \bo{\phi}\_k)^{z\_{d, n, k}} \right>_{q(\bo{\Phi})} \newline
&= \left< \sum\_{k = 1}^K z\_{d, n, k} \ln \mathrm{Cat} (\b{w}\_{d, n} | \bo{\phi}\_k) \right>\_{q(\bo{\Phi})} \newline
&= \sum\_{k = 1}^K z\_{d, n, k} \sum\_{v = 1}^V w\_{d, n, v} \left< \ln \phi\_{k, v} \right>
\end{align}

\begin{align}
\left< \ln p(\b{z}\_{d, n} | \bo{\theta}_d ) \right>\_{q(\bo{\theta}_d)} = \left< \ln \mathrm{Cat} (\b{z}\_{d, n} | \bo{\theta}_d ) \right>\_{q(\bo{\theta}_d)} = \sum\_{k = 1}^K z\_{d, n, k} \left< \ln \theta\_{d, k} \right>
\end{align}

と計算できます．したがって

\begin{align}
\ln q(\b{z}\_{d, n}) 
&= \sum\_{k = 1}^K z\_{d, n, k} \sum\_{v = 1}^V w\_{d, n, v} \left< \ln \phi\_{k, v} \right> + \sum\_{k = 1}^K z\_{d, n, k} \left< \ln \theta\_{d, k} \right> + \mathrm{const.} \newline
&= \sum\_{k = 1}^K z\_{d, n, k} \left\\{ \sum\_{v = 1}^V w\_{d, n, v} \left< \ln \phi\_{k, v} \right> + \left< \ln \theta\_{d, k} \right> \right\\} + \mathrm{const.}
\end{align}

となり，以下のカテゴリ分布になることが分かります．

\begin{align}
q(\b{z}\_{d, n}) = \mathrm{Cat} (\b{z}\_{d, n} | \bo{\eta}\_{d, n})
\end{align}

\begin{align}
\eta\_{d, n, k} \propto \exp \left\\{ \sum\_{v = 1}^V w\_{d, n, v} \left< \ln \phi\_{k, v} \right> + \left< \ln \theta\_{d, k} \right> \right\\} \; \left( \mathrm{s.t.} \sum_{k = 1}^K \eta\_{k, n, k} = 1 \right)
\end{align}

次は$q(\bo{\Theta}, \bo{\Phi})$です．ただこれは$\bo{\Theta}, \bo{\Phi}$が1つの分布に同時に出てきていないので，各変数について分けて分布を求めることができます．$q(\bo{\Theta})$は

\begin{align}
\ln q(\bo{\Theta}) 
&= \left< \ln p(\b{Z} | \bo{\Theta}) \right>\_{q(\b{Z})} + \ln p(\bo{\Theta}) + \mathrm{const.} \newline
&= \sum\_{d = 1}^D \sum\_{n = 1}^N \sum\_{k = 1}^K \left< z\_{d, n, k} \right> \ln \theta\_{d, k} + \sum\_{d = 1}^D \sum\_{k = 1}^K (\alpha_k - 1) \ln \theta\_{d, k} + \mathrm{const.} \newline
&= \sum\_{d = 1}^D \sum\_{k = 1}^K \left( \sum\_{n = 1}^N \left< z\_{d, n, k} \right> + \alpha_k - 1 \right) \ln \theta\_{d, k}
\end{align}

となり，$\sum\_{k = 1}^K \theta\_{d, k} = 1$の制約からディリクレ分布になることが分かります．

\begin{align}
q(\bo{\Theta}) = \prod\_{d = 1}^D \mathrm{Dir} (\bo{\theta}_d | \hat{\bo{\alpha}}_d) \quad \left(\hat{\alpha}\_{d, k} = \sum\_{n = 1}^N \left< z\_{d, n, k} \right> + \alpha_k \right)
\end{align}

$q(\bo{\Phi})$は

\begin{align}
\ln q(\bo{\Phi}) 
&= \left< \ln p(\b{W} | \b{Z}, \bo{\Phi}) \right>_{q(\b{Z})} + \ln p(\bo{\Phi}) + \mathrm{const.} \newline
&= \sum\_{d = 1}^D \sum\_{n = 1}^N \left< \ln p(\b{w}\_{d, n} | \b{z}\_{d, n}, \bo{\Phi}) \right>\_{q(\b{z}\_{d, n})} + \sum\_{k = 1}^K \ln p(\bo{\phi}_k) + \mathrm{const.} \newline
&= \sum\_{d = 1}^D \sum\_{n = 1}^N \sum\_{k = 1}^K \left< z\_{d, n, k} \right> \sum\_{v = 1}^V w\_{d, n, v} \ln \phi\_{k, v} + \sum\_{k = 1}^K \sum\_{v = 1}^V (\beta_v - 1) \ln \phi\_{k, v} + \mathrm{const.} \newline
&= \sum\_{k = 1}^K \sum\_{v = 1}^V \sum\_{d = 1}^D \sum\_{n = 1}^N \left< z\_{d, n, k} \right> w\_{d, n, v} \ln \phi\_{k, v} + \sum\_{k = 1}^K \sum\_{v = 1}^V (\beta_v - 1) \ln \phi\_{k, v} + \mathrm{const.} \newline
&= \sum\_{k = 1}^K \sum\_{v = 1}^V \left( \sum\_{d = 1}^D \sum\_{n = 1}^N \left< z\_{d, n, k} \right> w\_{d, n, v} + \beta_v - 1 \right) \ln \phi\_{k, v} + \mathrm{const.}
\end{align}

となるので，$\bo{\phi}_k$毎に独立して考えることができます．$q(\bo{\phi}_k)$は上記の結果からディリクレ分布となることが分かります．

\begin{align}
q(\bo{\phi}_k) = \mathrm{Dir} (\bo{\phi}_k | \hat{\bo{\beta}}_k) \quad \left( \hat{\beta}\_{k, v} = \sum\_{d = 1}^D \sum\_{n = 1}^N \left< z\_{d, n, k} \right> w\_{d, n, v} + \beta_v \right)
\end{align}

以上より，近似した事後分布が求められたので，期待値は以下の通りになります．

\begin{align}
\left< \ln \phi\_{k, v} \right> &= \psi \left( \hat{\beta}\_{k, v} \right) - \psi \left( \sum\_{v' = 1}^V \hat{\beta}\_{k, v'} \right) \newline
\left< \ln \theta\_{d, k} \right> &= \psi \left( \hat{\alpha}\_{d, k} \right) - \psi \left( \sum\_{k' = 1}^K \hat{\alpha}\_{d, k'} \right) \newline
\left< z\_{d, n, k} \right> &= \eta\_{d, n, k}
\end{align}

## References

- [ベイズ推論による機械学習入門](https://www.kspub.co.jp/book/detail/1538320.html)

[bayes-book]: https://www.kspub.co.jp/book/detail/1538320.html
