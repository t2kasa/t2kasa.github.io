---
title:  "SVM (1)"
permalink: 2018-12-16-svm-part1.html
sidebar: blog_sidebar
tags: [SVM]
---

\begin{align}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}} \nonumber
\end{align}

SVMを復習しよう．ふとそう思った．今回は最も基本的なケースであるハードマージンかつ線形のSVMについて書く．これは2クラス分類問題において，ある線形判別関数$g(\bx)$によって線形分離可能であることを仮定している．
ハードマージンSVMの詳細については後で更新が確認しやすいように，ブログとは別の[こちらのページ](hard-margin-svm.html)に書いてあるので，まずはそちらを見て頂きたい．

### CVXOPTによる実装例

凸最適化のためのパッケージ[CVXOPT](https://github.com/cvxopt/cvxopt)で$\balpha$を求めてみよう．以下，コードと実行結果の例である．赤丸で囲んでいるサンプルは$\alpha_i > 0$，即ちサポートベクトルに対応するサンプルである．

<script src="https://gist.github.com/t2kasa/32b663cde01415f633f0f3720f57935f.js"></script>
![example](images/figs/2018-12-16-svm-part1/example.png)

### 参考文献

* [パターン認識と機械学習 下](https://www.maruzen-publishing.co.jp/item/b294551.html)
* [最適化と変分法](https://www.maruzen-publishing.co.jp/item/b294841.html)
